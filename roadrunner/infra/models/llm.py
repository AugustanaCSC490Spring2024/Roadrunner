import os

from litellm import completion
from openai import OpenAI

from ..prompts import summary_prompt


class LLMClient:
    def __init__(self):
        self.client = OpenAI()
        self.api_key = self._get_api_key()
        self.conversation = self._initialize_conversation()

    def _get_api_key(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "API key not found. Please set the OPENAI_API_KEY environment variable."
            )
        return api_key

    def _initialize_conversation(self):
        return [{"role": "system", "content": summary_prompt()}]

    def add_message(self, role: str, content: str):
        self.conversation.append({"role": role, "content": content})

    def generate_embeddings(self, text):
        """
        Generates embeddings for the given text using the LLM model.

        Args:
        - text (str): The text to generate embeddings for.

        Returns:
        - The embeddings generated by the LLM model.
        """
        self.add_message("system", "Generating embeddings for the given text.")
        try:
            response = self.client.embeddings.create(model="text-embedding-ada-002", input=text)
            embeddings = response.data[0].embedding if response.data else None
            if embeddings is None:
                raise ValueError("No embeddings generated")
            return embeddings
        except Exception as e:
            print(f"An error occurred while generating embeddings: {e}")
            return None

    def generate_completion(self, user_message):
        self.add_message("user", user_message)
        print(self.conversation)
        response = completion(model="gpt-3.5-turbo", messages=self.conversation, api_key=self.api_key)
        return response.get('choices', [{}])[0].get('message', {}).get('content', 'No response generated')

if __name__ == "__main__":
    llm_client = LLMClient()
    print(llm_client.generate_completion("Test input"))
