import os
import traceback

from litellm import acompletion, completion
from openai import OpenAI

from roadrunner.infra.utils import logger

from ..prompts import summary_prompt

log = logger.get_logger(__name__)


class LLMClient:
    def __init__(self):
        self.client = OpenAI()
        self.api_key = self._get_api_key()

    def _get_api_key(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "API key not found. Please set the OPENAI_API_KEY environment variable."
            )
        return api_key

    def get_system_message(self):
        return [{"role": "system", "content": summary_prompt()}]

    def generate_embeddings(self, text):
        """
        Generates embeddings for the given text using the LLM model.

        Args:
        - text (str): The text to generate embeddings for.

        Returns:
        - The embeddings generated by the LLM model.
        """
        self.add_message("system", "Generating embeddings for the given text.")
        try:
            response = self.client.embeddings.create(
                model="text-embedding-ada-002", input=text
            )
            embeddings = response.data[0].embedding if response.data else None
            if embeddings is None:
                raise ValueError("No embeddings generated")
            return embeddings
        except Exception as e:
            print(f"An error occurred while generating embeddings: {e}")
            return None

    async def async_completion(self, messages):
        log.info(f"LLM async completion request...")
        messages = self.get_system_message() + messages
        log.info(f"Conversation: {messages}")

        return await acompletion(
            model="gpt-3.5-turbo",
            messages=messages,
            api_key=self.api_key,
            stream=True,
        )


if __name__ == "__main__":
    llm_client = LLMClient()
    print(llm_client.generate_completion("Test input"))
