import logging
import os
import traceback

from litellm import acompletion, completion
from openai import OpenAI

from ..prompts import summary_prompt

logger = logging.getLogger(__name__)


class LLMClient:
    def __init__(self):
        self.client = OpenAI()
        self.api_key = self._get_api_key()
        self.conversation = self._initialize_conversation()

    def _get_api_key(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "API key not found. Please set the OPENAI_API_KEY environment variable."
            )
        return api_key

    def _initialize_conversation(self):
        return [{"role": "system", "content": summary_prompt()}]

    def add_message(self, role: str, content: str):
        self.conversation.append({"role": role, "content": content})

    def generate_embeddings(self, text):
        """
        Generates embeddings for the given text using the LLM model.

        Args:
        - text (str): The text to generate embeddings for.

        Returns:
        - The embeddings generated by the LLM model.
        """
        self.add_message("system", "Generating embeddings for the given text.")
        try:
            response = self.client.embeddings.create(
                model="text-embedding-ada-002", input=text
            )
            embeddings = response.data[0].embedding if response.data else None
            if embeddings is None:
                raise ValueError("No embeddings generated")
            return embeddings
        except Exception as e:
            print(f"An error occurred while generating embeddings: {e}")
            return None

    async def async_completion(self, user_message):
        logger.info(f"LLM async completion request...")
        self.add_message("user", user_message)
        logger.info(f"Conversation: {self.conversation}")
        return await acompletion(
            model="gpt-3.5-turbo",
            messages=self.conversation,
            api_key=self.api_key,
            stream=True,
        )


if __name__ == "__main__":
    llm_client = LLMClient()
    print(llm_client.generate_completion("Test input"))
